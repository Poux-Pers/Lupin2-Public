{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- LIBRARIES -----\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import cryptocompare\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "with open(os.getcwd()+'\\\\parameters\\\\Parameters.json', 'r') as json_file:\n",
    "    Parameters = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "\n",
    "    def __init__(self, Parameters):\n",
    "        self.mesh = Parameters['Mesh']\n",
    "        self.hist = pd.DataFrame([])\n",
    "        self.hist_path = Parameters['hist_path']\n",
    "        self.ML_dataset_path = Parameters['ML_dataset_path']\n",
    "        self.trend_length = Parameters['trend_length']\n",
    "        self.ML_trend_length = Parameters['ML_trend_length']\n",
    "        self.parameters = Parameters\n",
    "        self.date_name = ''\n",
    "        self.companies_list_path = Parameters['Companies_list_path']\n",
    "        self.companies_list = pd.read_csv(os.getcwd() +Parameters['Companies_list_path'])['Companies'].to_list()\n",
    "        self.study_length = Parameters['study_length']\n",
    "        self.LSTM_model_path = Parameters['LSTM_model_path']\n",
    "\n",
    "    def load(self):\n",
    "        \n",
    "        if self.mesh == '1m':\n",
    "            self.path = os.getcwd() + '\\\\resources\\\\full_NASDAQ_history_1m.csv'\n",
    "            self.date_name = 'Datetime'\n",
    "            self.hist = pd.read_csv(self.path, usecols=['Close', 'Company', 'Datetime', 'Dividends', 'High', 'Low', 'Open', 'Stock Splits', 'Volume'])\n",
    "\n",
    "        else:\n",
    "            self.path = os.getcwd() + self.hist_path\n",
    "            self.date_name = 'Date'\n",
    "            self.hist = pd.read_csv(self.path, usecols=['Close', 'Company', 'Date', 'Dividends', 'High', 'Low', 'Open', 'Stock Splits', 'Volume'])\n",
    "        \n",
    "        # Date formating\n",
    "        return(self.hist)\n",
    "\n",
    "    def update(self, update_period='max'):\n",
    "        # Initialization\n",
    "        list_of_df_to_merge = [self.hist]\n",
    "\n",
    "        for company in self.companies_list:\n",
    "            # Get history Ignore timezone\n",
    "            hist = yf.Ticker(company).history(period=update_period, interval=self.mesh)\n",
    "\n",
    "            if hist.empty != True:\n",
    "                # Reset index, add company name and format date\n",
    "                hist = hist.reset_index()\n",
    "                hist['Company'] = [company]*len(hist)\n",
    "\n",
    "                if self.date_name == '1m':                \n",
    "                    hist[self.date_name] = hist[self.date_name].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    hist[self.date_name] = hist[self.date_name].dt.floor('min')\n",
    "                else:\n",
    "                    hist[self.date_name] = hist[self.date_name].astype('datetime64[ns]')\n",
    "                \n",
    "                hist[self.date_name] = pd.to_datetime(hist[self.date_name])\n",
    "                \n",
    "                # Add hist to the list of dict to merge\n",
    "                list_of_df_to_merge.append(hist)\n",
    "\n",
    "            else:\n",
    "                # We remove companies whose data was not available\n",
    "                self.companies_list.remove(company)\n",
    "\n",
    "        # Concat and remove duplicates\n",
    "        new_hist = pd.concat(list_of_df_to_merge)[self.hist.columns]\n",
    "        new_hist = new_hist.drop_duplicates(subset=[self.date_name, 'Company'], keep='last')\n",
    "\n",
    "        # reset index for the new dataframe\n",
    "        new_hist = new_hist.reset_index(drop=True)\n",
    "        new_hist\n",
    "\n",
    "        # Update Company list\n",
    "        with open(os.getcwd() + self.companies_list_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(zip(['Companies']+self.companies_list))\n",
    "\n",
    "        self.hist = new_hist\n",
    "        self.save()\n",
    "\n",
    "        return(self.hist)\n",
    "\n",
    "    def update_crypto(self, cryptoname):\n",
    "        # Fetch info\n",
    "        btc_hist = cryptocompare.get_historical_price_day(cryptoname, curr='USD', limit=2000)\n",
    "\n",
    "        # Load\n",
    "        df_hist = pd.DataFrame(btc_hist)\n",
    "\n",
    "        # Time format\n",
    "        df_hist['time'] = pd.to_datetime(df_hist['time'], unit='s')\n",
    "\n",
    "        # Rename\n",
    "        df_hist = df_hist.rename(columns={\"time\": \"Date\", \"open\": \"Open\", \"high\": \"High\", \"low\": \"Low\", \"close\": \"Close\", \"volumeto\": \"Volume\"})\n",
    "\n",
    "        # Reorder columns\n",
    "        df_hist = df_hist[['Date','Open','High','Low','Close','Volume']]\n",
    "\n",
    "        # Completion with fake values\n",
    "        df_hist['Dividends'] = len(df_hist) * [0]\n",
    "        df_hist['Stock Splits'] = len(df_hist) * [0]\n",
    "        df_hist['Company'] = len(df_hist) * [cryptoname]\n",
    "\n",
    "        # Concat and remove duplicates\n",
    "        new_hist = pd.concat([self.hist, df_hist])[self.hist.columns]\n",
    "        new_hist = new_hist.drop_duplicates(subset=[self.date_name, 'Company'], keep='last')\n",
    "\n",
    "        # reset index for the new dataframe\n",
    "        new_hist = new_hist.reset_index(drop=True)\n",
    "        new_hist\n",
    "\n",
    "        # Update Company list\n",
    "        with open(os.getcwd() + self.companies_list_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(zip(['Companies']+self.companies_list))\n",
    "\n",
    "        self.hist = new_hist\n",
    "        self.save()\n",
    "\n",
    "        return(self.hist)\n",
    "\n",
    "    def save(self):\n",
    "        self.hist.to_csv(self.path)\n",
    "\n",
    "    def new_format(self, study_length):\n",
    "        # TCD to set date in columns, have a sum of the companies\n",
    "        TCD = pd.pivot_table(self.hist, 'Open', index=['Company'], columns=[self.date_name], aggfunc=np.sum, margins=True, margins_name='NASDAQ').fillna(method='ffill', axis=1)\n",
    "        \n",
    "        # Keeping only the NASDAQ row\n",
    "        TCD = TCD.drop(columns=['NASDAQ'])\n",
    "\n",
    "        # Sorting columns\n",
    "        TCD = TCD.reindex(TCD.columns.tolist().sort(), axis=1)\n",
    "        \n",
    "        # Replacing remaining NaN by 0 \n",
    "        #TCD.fillna(0)\n",
    "\n",
    "        # Reshaping\n",
    "        TCD.columns.name = None\n",
    "        TCD = TCD.reset_index().rename_axis(None, axis=1).set_index('Company')\n",
    "\n",
    "        # Resizing\n",
    "        dataset = TCD[TCD.columns[-study_length:]]\n",
    "\n",
    "        return(dataset)\n",
    "\n",
    "    def create_ML_dataset(self, dataset):\n",
    "        datasets_list = []\n",
    "        # Reducing the dataset only to the companies in the list\n",
    "        dataset = dataset[dataset.index.isin(self.companies_list)]\n",
    "        dataset = dataset.reset_index()\n",
    "\n",
    "        # Dataset enrichment\n",
    "        #supplement_df = self.enrich_symbol(['sector', 'country', 'shortName'])\n",
    "        #supplement_df = supplement_df.reset_index()\n",
    "\n",
    "        # Columns creation\n",
    "        columns = ['Company']\n",
    "        for i in range(self.ML_trend_length):\n",
    "            columns.append('Day_'+str(i+1))\n",
    "        columns.append('prediction')\n",
    "\n",
    "        # Let's not take the period we study for training :)\n",
    "        for day in range(self.ML_trend_length+1, len(dataset.columns.to_list())-self.study_length):\n",
    "            # Reinitialization\n",
    "            BS_dict_list = []\n",
    "            prediction_dict_list = []\n",
    "\n",
    "            # Reducing the dataset to the trend period studied and the companies in the companies list TODO add string integration for ML \n",
    "            small_dataset = dataset[['Company']+dataset.columns[day-self.ML_trend_length:day+1].to_list()]\n",
    "            #small_dataset = dataset[+dataset.columns[day-self.ML_trend_length:day+1].to_list()]\n",
    "\n",
    "            # Rename columns\n",
    "            small_dataset.columns = columns\n",
    "\n",
    "            # One Hot Encoding to add companies as feature\n",
    "            Company_features = pd.get_dummies(small_dataset.Company, prefix='Company')\n",
    "            small_dataset = pd.concat([Company_features, small_dataset], axis=1)\n",
    "\n",
    "            datasets_list.append(small_dataset)\n",
    "\n",
    "        # Add columns TODO add string integration for ML\n",
    "        #columns += ['sector', 'country', 'shortName']\n",
    "\n",
    "        # Enrich the data frame with feature\n",
    "        ML_dataset = pd.concat(datasets_list).dropna()\n",
    "        #ML_dataset = ML_dataset.join(supplement_df.set_index('index'), on='Company')\n",
    "        \n",
    "        #Remove the 'Company Column'\n",
    "        ML_dataset.pop('Company')\n",
    "        \n",
    "        # Keep all Company columns out to avoid losing the 1\n",
    "        temp_ML_Dataset = ML_dataset[ML_dataset.columns.to_list()[0:len(self.companies_list)]].copy()\n",
    "        ML_dataset = ML_dataset.drop(ML_dataset.columns.to_list()[0:len(self.companies_list)], axis=1)\n",
    "        # Remove row with a 0 as value\n",
    "        ML_dataset = ML_dataset.loc[ML_dataset.Day_1 > 0]\n",
    "        # Normalize lines\n",
    "        ML_dataset = ML_dataset.div(ML_dataset.max(axis=1)*2, axis=0)\n",
    "        # Concatenate it again\n",
    "        ML_dataset = pd.concat([temp_ML_Dataset, ML_dataset], axis=1).dropna()\n",
    "        \n",
    "        # normalize the dataset\n",
    "        #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        #dataset = scaler.fit_transform(dataset)\n",
    "        \n",
    "        # Save dataframe \n",
    "        ML_dataset.to_csv(os.getcwd() + self.ML_dataset_path)\n",
    "\n",
    "        return(ML_dataset)\n",
    "    \n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return(np.array(dataX), np.array(dataY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195956\n"
     ]
    }
   ],
   "source": [
    "# Test lines, executed only when the file is executed as main\n",
    "full_hist = Dataset(Parameters)\n",
    "full_hist.load()\n",
    "\n",
    "if full_hist.date_name != '1m':\n",
    "    full_hist.hist[full_hist.date_name] = full_hist.hist[full_hist.date_name].astype('datetime64[ns]')\n",
    "else:\n",
    "    full_hist.hist[full_hist.date_name] = full_hist.hist[full_hist.date_name].dt.floor('min')\n",
    "\n",
    "full_hist.update_crypto('ETH')\n",
    "#full_hist.save()\n",
    "#print(full_hist.hist)\n",
    "print(len(full_hist.hist))\n",
    "dataset = full_hist.new_format(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41978145]\n",
      " [0.4309855 ]\n",
      " [0.4373361 ]\n",
      " [0.4276533 ]\n",
      " [0.4276533 ]\n",
      " [0.4276533 ]\n",
      " [0.37607622]\n",
      " [0.38498628]\n",
      " [0.3501668 ]\n",
      " [0.33705533]\n",
      " [0.27948982]\n",
      " [0.27948982]\n",
      " [0.27948982]\n",
      " [0.33990455]\n",
      " [0.39155412]\n",
      " [0.3740962 ]\n",
      " [0.37187457]\n",
      " [0.33922845]\n",
      " [0.33922845]\n",
      " [0.33922845]\n",
      " [0.2951609 ]\n",
      " [0.32749325]\n",
      " [0.32809693]\n",
      " [0.27630246]\n",
      " [0.29791367]\n",
      " [0.29791367]\n",
      " [0.29791367]\n",
      " [0.2425214 ]\n",
      " [0.25594687]\n",
      " [0.23725748]\n",
      " [0.25565714]\n",
      " [0.25515002]\n",
      " [0.25515002]\n",
      " [0.25515002]\n",
      " [0.20903009]\n",
      " [0.22902346]\n",
      " [0.2637704 ]\n",
      " [0.25355637]\n",
      " [0.2685997 ]\n",
      " [0.2685997 ]\n",
      " [0.2685997 ]\n",
      " [0.2637462 ]\n",
      " [0.27548146]\n",
      " [0.25350803]\n",
      " [0.23863375]\n",
      " [0.24457389]\n",
      " [0.24457389]\n",
      " [0.24457389]\n",
      " [0.2641325 ]\n",
      " [0.31218433]\n",
      " [0.2927221 ]\n",
      " [0.3071136 ]\n",
      " [0.3071136 ]\n",
      " [0.3071136 ]\n",
      " [0.3071136 ]\n",
      " [0.30617172]\n",
      " [0.33439904]\n",
      " [0.34019434]\n",
      " [0.35221928]\n",
      " [0.34572393]\n",
      " [0.34572393]\n",
      " [0.34572393]\n",
      " [0.32944906]\n",
      " [0.32541662]\n",
      " [0.31896943]\n",
      " [0.32442665]\n",
      " [0.32763815]\n",
      " [0.32763815]\n",
      " [0.32763815]\n",
      " [0.33874553]\n",
      " [0.34666544]\n",
      " [0.34582043]\n",
      " [0.3584491 ]\n",
      " [0.34949082]\n",
      " [0.34949082]\n",
      " [0.34949082]\n",
      " [0.35654157]\n",
      " [0.3707639 ]\n",
      " [0.38380307]\n",
      " [0.39046752]\n",
      " [0.39830887]\n",
      " [0.39830887]\n",
      " [0.39830887]\n",
      " [0.40426493]\n",
      " [0.42782307]\n",
      " [0.41407079]\n",
      " [0.39557284]\n",
      " [0.38550067]\n",
      " [0.38550067]\n",
      " [0.38550067]\n",
      " [0.41654038]\n",
      " [0.42104375]\n",
      " [0.42503875]\n",
      " [0.4298327 ]\n",
      " [0.42283547]\n",
      " [0.42283547]\n",
      " [0.42283547]\n",
      " [0.42283547]\n",
      " [0.44155127]\n",
      " [0.42373133]\n",
      " [0.42525667]\n",
      " [0.4312613 ]\n",
      " [0.4312613 ]\n",
      " [0.4312613 ]\n",
      " [0.42762947]\n",
      " [0.43489307]\n",
      " [0.44435996]\n",
      " [0.4437062 ]\n",
      " [0.44118822]\n",
      " [0.44118822]\n",
      " [0.44118822]\n",
      " [0.45789438]\n",
      " [0.46247047]\n",
      " [0.5006285 ]\n",
      " [0.5040424 ]\n",
      " [0.4929291 ]\n",
      " [0.4929291 ]\n",
      " [0.4929291 ]\n",
      " [0.46515793]\n",
      " [0.5092479 ]\n",
      " [0.5181822 ]\n",
      " [0.50912696]\n",
      " [0.5169474 ]\n",
      " [0.5169474 ]\n",
      " [0.5169474 ]\n",
      " [0.5089574 ]\n",
      " [0.53960973]\n",
      " [0.5420309 ]\n",
      " [0.5316198 ]\n",
      " [0.54060245]\n",
      " [0.54060245]\n",
      " [0.54060245]\n",
      " [0.51358193]\n",
      " [0.5301186 ]\n",
      " [0.5423215 ]\n",
      " [0.54893136]\n",
      " [0.54893136]\n",
      " [0.54893136]\n",
      " [0.54893136]\n",
      " [0.55413693]\n",
      " [0.5672356 ]\n",
      " [0.57040733]\n",
      " [0.5905759 ]\n",
      " [0.5815933 ]\n",
      " [0.5815933 ]\n",
      " [0.5815933 ]\n",
      " [0.60028493]\n",
      " [0.5767993 ]\n",
      " [0.61699116]\n",
      " [0.59348136]\n",
      " [0.5975975 ]\n",
      " [0.5975975 ]\n",
      " [0.5975975 ]\n",
      " [0.59207714]\n",
      " [0.61875856]\n",
      " [0.59474033]\n",
      " [0.5976942 ]\n",
      " [0.53948873]\n",
      " [0.53948873]\n",
      " [0.53948873]\n",
      " [0.56585544]\n",
      " [0.57222325]\n",
      " [0.5662429 ]\n",
      " [0.57048   ]\n",
      " [0.65471345]\n",
      " [0.65471345]\n",
      " [0.65471345]\n",
      " [0.7061879 ]\n",
      " [0.7152191 ]\n",
      " [0.7175918 ]\n",
      " [0.7275428 ]\n",
      " [0.75663704]\n",
      " [0.75663704]\n",
      " [0.75663704]\n",
      " [0.7507672 ]\n",
      " [0.7446547 ]\n",
      " [0.7303682 ]\n",
      " [0.7685222 ]\n",
      " [0.7724032 ]\n",
      " [0.7724032 ]\n",
      " [0.7724032 ]\n",
      " [0.7843612 ]\n",
      " [0.76777035]\n",
      " [0.783585  ]\n",
      " [0.7813292 ]\n",
      " [0.8154083 ]\n",
      " [0.8154083 ]\n",
      " [0.8154083 ]\n",
      " [0.906949  ]\n",
      " [0.86814004]\n",
      " [0.8825236 ]\n",
      " [0.8918621 ]\n",
      " [0.88089854]\n",
      " [0.88089854]\n",
      " [0.88089854]\n",
      " [0.8961068 ]\n",
      " [0.9463642 ]\n",
      " [0.9932261 ]\n",
      " [0.8896063 ]\n",
      " [0.82324296]\n",
      " [0.82324296]\n",
      " [0.82324296]\n",
      " [0.82324296]\n",
      " [0.7638652 ]\n",
      " [0.7959797 ]\n",
      " [0.82605654]\n",
      " [0.7698806 ]\n",
      " [0.7698806 ]\n",
      " [0.7698806 ]\n",
      " [0.7713359 ]\n",
      " [0.806361  ]\n",
      " [0.77628416]\n",
      " [0.72282475]\n",
      " [0.7294223 ]\n",
      " [0.7294223 ]\n",
      " [0.7294223 ]\n",
      " [0.67256707]\n",
      " [0.75154334]\n",
      " [0.7412589 ]\n",
      " [0.6786795 ]\n",
      " [0.71030885]\n",
      " [0.71030885]\n",
      " [0.71030885]\n",
      " [0.7741496 ]\n",
      " [0.7696865 ]\n",
      " [0.7623128 ]\n",
      " [0.79966646]\n",
      " [0.75358087]\n",
      " [0.75358087]\n",
      " [0.75358087]\n",
      " [0.76347715]\n",
      " [0.78084415]\n",
      " [0.7703658 ]\n",
      " [0.7861803 ]\n",
      " [0.7767692 ]\n",
      " [0.7767692 ]\n",
      " [0.7767692 ]\n",
      " [0.8231459 ]\n",
      " [0.8736946 ]\n",
      " [0.83226603]\n",
      " [0.81014496]\n",
      " [0.8349826 ]\n",
      " [0.8349826 ]\n",
      " [0.8349826 ]\n",
      " [0.8221757 ]\n",
      " [0.78569525]\n",
      " [0.79025525]\n",
      " [0.797823  ]\n",
      " [0.7875387 ]\n",
      " [0.7875387 ]\n",
      " [0.7875387 ]\n",
      " [0.7644474 ]\n",
      " [0.7788066 ]\n",
      " [0.7745376 ]\n",
      " [0.7485357 ]\n",
      " [0.7358257 ]\n",
      " [0.7358257 ]\n",
      " [0.7358257 ]\n",
      " [0.71690637]\n",
      " [0.72224265]\n",
      " [0.7657086 ]\n",
      " [0.8026741 ]\n",
      " [0.8082445 ]\n",
      " [0.8082445 ]\n",
      " [0.8082445 ]\n",
      " [0.82943195]\n",
      " [0.781323  ]\n",
      " [0.79726213]\n",
      " [0.8208793 ]\n",
      " [0.81912977]\n",
      " [0.81912977]\n",
      " [0.81912977]\n",
      " [0.8140759 ]\n",
      " [0.82019895]\n",
      " [0.811063  ]\n",
      " [0.80114967]\n",
      " [0.8113546 ]\n",
      " [0.8113546 ]\n",
      " [0.8113546 ]\n",
      " [0.79716486]\n",
      " [0.7653839 ]\n",
      " [0.781323  ]\n",
      " [0.781323  ]\n",
      " [0.79123634]\n",
      " [0.79123634]\n",
      " [0.79123634]\n",
      " [0.7951239 ]\n",
      " [0.83438855]\n",
      " [0.8442047 ]\n",
      " [0.8587832 ]\n",
      " [0.8498418 ]\n",
      " [0.8498418 ]\n",
      " [0.8498418 ]\n",
      " [0.8470232 ]\n",
      " [0.8670444 ]\n",
      " [0.86859936]\n",
      " [0.82943195]\n",
      " [0.84818953]\n",
      " [0.84818953]\n",
      " [0.84818953]\n",
      " [0.8498418 ]\n",
      " [0.8667528 ]\n",
      " [0.89659005]\n",
      " [0.91107124]\n",
      " [0.91165453]\n",
      " [0.91165453]\n",
      " [0.91165453]\n",
      " [0.87336165]\n",
      " [0.9374098 ]\n",
      " [0.9427553 ]\n",
      " [0.93459135]\n",
      " [0.93459135]\n",
      " [0.93459135]\n",
      " [0.93459135]\n",
      " [0.96054095]\n",
      " [1.        ]\n",
      " [0.97599417]\n",
      " [0.9614157 ]\n",
      " [0.9614157 ]\n",
      " [0.9614157 ]\n",
      " [0.9614157 ]\n",
      " [0.9559731 ]\n",
      " [0.9109742 ]\n",
      " [0.89960295]\n",
      " [0.90582305]\n",
      " [0.9453793 ]\n",
      " [0.9453793 ]\n",
      " [0.9453793 ]]\n",
      "(328, 1, 1)\n",
      "Epoch 1/50\n",
      "668/668 - 2s - loss: 0.0069 - mean_absolute_error: 0.0621\n",
      "Epoch 2/50\n",
      "668/668 - 0s - loss: 0.0024 - mean_absolute_error: 0.0367\n",
      "Epoch 3/50\n",
      "668/668 - 0s - loss: 4.0869e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 4/50\n",
      "668/668 - 0s - loss: 5.8876e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 5/50\n",
      "668/668 - 0s - loss: 5.1325e-05 - mean_absolute_error: 0.0048\n",
      "Epoch 6/50\n",
      "668/668 - 0s - loss: 5.2464e-05 - mean_absolute_error: 0.0049\n",
      "Epoch 7/50\n",
      "668/668 - 1s - loss: 5.1771e-05 - mean_absolute_error: 0.0049\n",
      "Epoch 8/50\n",
      "668/668 - 0s - loss: 5.2446e-05 - mean_absolute_error: 0.0050\n",
      "Epoch 9/50\n",
      "668/668 - 0s - loss: 5.4479e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 10/50\n",
      "668/668 - 1s - loss: 5.5776e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 11/50\n",
      "668/668 - 0s - loss: 5.5863e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 12/50\n",
      "668/668 - 1s - loss: 5.8374e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 13/50\n",
      "668/668 - 0s - loss: 5.3998e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 14/50\n",
      "668/668 - 0s - loss: 5.3584e-05 - mean_absolute_error: 0.0050\n",
      "Epoch 15/50\n",
      "668/668 - 0s - loss: 5.3605e-05 - mean_absolute_error: 0.0050\n",
      "Epoch 16/50\n",
      "668/668 - 0s - loss: 5.4378e-05 - mean_absolute_error: 0.0050\n",
      "Epoch 17/50\n",
      "668/668 - 0s - loss: 5.6446e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 18/50\n",
      "668/668 - 0s - loss: 5.6732e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 19/50\n",
      "668/668 - 0s - loss: 5.5967e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 20/50\n",
      "668/668 - 0s - loss: 5.7148e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 21/50\n",
      "668/668 - 0s - loss: 5.5794e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 22/50\n",
      "668/668 - 1s - loss: 5.7054e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 23/50\n",
      "668/668 - 1s - loss: 5.8113e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 24/50\n",
      "668/668 - 1s - loss: 5.9896e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 25/50\n",
      "668/668 - 1s - loss: 5.5679e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 26/50\n",
      "668/668 - 1s - loss: 5.4156e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 27/50\n",
      "668/668 - 1s - loss: 5.7749e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 28/50\n",
      "668/668 - 1s - loss: 5.6103e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 29/50\n",
      "668/668 - 1s - loss: 5.4696e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 30/50\n",
      "668/668 - 1s - loss: 5.5841e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 31/50\n",
      "668/668 - 1s - loss: 5.6875e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 32/50\n",
      "668/668 - 1s - loss: 5.5258e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 33/50\n",
      "668/668 - 1s - loss: 5.5843e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 34/50\n",
      "668/668 - 1s - loss: 5.4639e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 35/50\n",
      "668/668 - 1s - loss: 5.6223e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 36/50\n",
      "668/668 - 1s - loss: 5.4229e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 37/50\n",
      "668/668 - 1s - loss: 5.7235e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 38/50\n",
      "668/668 - 1s - loss: 5.4027e-05 - mean_absolute_error: 0.0050\n",
      "Epoch 39/50\n",
      "668/668 - 1s - loss: 5.5406e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 40/50\n",
      "668/668 - 1s - loss: 5.3913e-05 - mean_absolute_error: 0.0050\n",
      "Epoch 41/50\n",
      "668/668 - 1s - loss: 5.6991e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 42/50\n",
      "668/668 - 1s - loss: 5.7566e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 43/50\n",
      "668/668 - 1s - loss: 5.6099e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 44/50\n",
      "668/668 - 1s - loss: 5.4822e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 45/50\n",
      "668/668 - 1s - loss: 5.8409e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 46/50\n",
      "668/668 - 1s - loss: 5.6871e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 47/50\n",
      "668/668 - 1s - loss: 5.3224e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 48/50\n",
      "668/668 - 1s - loss: 5.6632e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 49/50\n",
      "668/668 - 1s - loss: 5.5000e-05 - mean_absolute_error: 0.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "668/668 - 1s - loss: 5.5593e-05 - mean_absolute_error: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_65_layer_call_and_return_conditional_losses, lstm_cell_65_layer_call_fn, lstm_cell_65_layer_call_fn, lstm_cell_65_layer_call_and_return_conditional_losses, lstm_cell_65_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_65_layer_call_and_return_conditional_losses, lstm_cell_65_layer_call_fn, lstm_cell_65_layer_call_fn, lstm_cell_65_layer_call_and_return_conditional_losses, lstm_cell_65_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\louis.poux\\OneDrive - BearingPoint GmbH\\5 - Python\\Codes\\Lupin2_perso\\models\\LSTM_AAPL\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\louis.poux\\OneDrive - BearingPoint GmbH\\5 - Python\\Codes\\Lupin2_perso\\models\\LSTM_AAPL\\assets\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-258f24fb6f23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mtestY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# calculate root mean squared error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mtrainScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainPredict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'_Train Score: %.2f RMSE'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainScore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mtestScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestPredict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "# Reducing the dataset only to the companies in the list\n",
    "dataset = dataset[dataset.index.isin(full_hist.companies_list)]\n",
    "\n",
    "for company in full_hist.companies_list:\n",
    "    values_list = dataset.loc[company,:].values.astype('float32').reshape(-1, 1)\n",
    "    \n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    values_list = scaler.fit_transform(values_list)\n",
    "    \n",
    "    # split into train and test sets\n",
    "    train_size = int(len(values_list) * 0.67)\n",
    "    test_size = len(values_list) - train_size\n",
    "    train, test = values_list[0:train_size,:], values_list[train_size:len(values_list),:]\n",
    "    #print(len(train), len(test))\n",
    "    \n",
    "    # reshape into X=t and Y=t+1\n",
    "    look_back = 1\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    print(testX)\n",
    "    \n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "    print(testX.shape)\n",
    "    \n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(), optimizer='adam', metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "    model.fit(trainX, trainY, epochs=50, batch_size=1, verbose=2)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(os.getcwd()+full_hist.LSTM_model_path+'_'+str(company))\n",
    "    \n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform([trainY])\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform([testY])\n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "    print(str(company)+ '_Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "    print(str(company)+ '_Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
